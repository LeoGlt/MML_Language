library(utils)
library(dplyr, warn.conflicts = FALSE)
library(caret)
library(e1071)
library(randomForest)
library(rpart)
library(rjson)
data <- read.csv("upload/iris.csv", sep=',')
y <- "variety"
X <- colnames(data %>% select(-"variety"))
formula = as.formula(paste(y, paste(X,collapse = " + "), sep = " ~ "))
results = list()
set <- sample(1:nrow(data),floor(0.8*nrow(data)))
dataTrain <- data[set,]
dataTest <- data[-set,]
results[[1]] = list(model=c(), output=list())
results[[1]]$model = "Random Forest"
clf = randomForest(y = as.factor(dataTrain[,y]), x = dataTrain[,X], ntree = 10)
y_pred=predict(clf,dataTest, type = "class")
mat_conf <- table(y_pred,unlist(dataTest %>% select(y)))
accuracy = sum(diag(mat_conf))/sum(mat_conf)
results[[1]]$output[[1]] = list(metric = "accuracy", value = accuracy)
recall = mat_conf[2,2]/sum(mat_conf[,2])
results[[1]]$output[[2]] = list(metric = "recall", value = recall)
precision = mat_conf[2,2]/sum(mat_conf[2,])
results[[1]]$output[[3]] = list(metric = "precision", value = precision)
precision = mat_conf[2,2]/sum(mat_conf[2,])
recall = mat_conf[2,2]/sum(mat_conf[,2])
F1 = 2*precision*recall/(precision+recall)
results[[1]]$output[[4]] = list(metric = "f1 score", value = F1)
 first_row = mat_conf[1,1] / (mat_conf[1,1] + mat_conf[1,2])  
 second_row <- mat_conf[2,2] / (mat_conf[2,1] + mat_conf[2,2])  
 balanced_accuracy = (first_row + second_row)/2
results[[1]]$output[[5]] = list(metric = "balanced accuracy", value = balanced_accuracy)
results[[2]] = list(model=c(), output=list())
results[[2]]$model = "Decision tree"
clf = rpart(formula = formula,data = dataTrain, method = "class",control = (maxdepth = 3),parms = list(split ="entropy"))
y_pred = predict(clf, dataTest, type = "class")
mat_conf <- table(y_pred,unlist(dataTest %>% select(y)))
accuracy = sum(diag(mat_conf))/sum(mat_conf)
results[[2]]$output[[1]] = list(metric = "accuracy", value = accuracy)
recall = mat_conf[2,2]/sum(mat_conf[,2])
results[[2]]$output[[2]] = list(metric = "recall", value = recall)
precision = mat_conf[2,2]/sum(mat_conf[2,])
results[[2]]$output[[3]] = list(metric = "precision", value = precision)
precision = mat_conf[2,2]/sum(mat_conf[2,])
recall = mat_conf[2,2]/sum(mat_conf[,2])
F1 = 2*precision*recall/(precision+recall)
results[[2]]$output[[4]] = list(metric = "f1 score", value = F1)
 first_row = mat_conf[1,1] / (mat_conf[1,1] + mat_conf[1,2])  
 second_row <- mat_conf[2,2] / (mat_conf[2,1] + mat_conf[2,2])  
 balanced_accuracy = (first_row + second_row)/2
results[[2]]$output[[5]] = list(metric = "balanced accuracy", value = balanced_accuracy)
results[[3]] = list(model=c(), output=list())
results[[3]]$model = "SVM"
if ("poly"=="rbf"){
clf = svm(formula, gamma= 1/length(X),C=1, kernel = "radial", data = dataTrain,  type = "C-classification")
}else{
clf = svm(formula, gamma= 1/length(X),C=1, kernel = "poly", data = dataTrain,  type = "C-classification")
}
y_pred=predict(clf,dataTest,type = "response")
mat_conf <- table(y_pred,unlist(dataTest %>% select(y)))
accuracy = sum(diag(mat_conf))/sum(mat_conf)
results[[3]]$output[[1]] = list(metric = "accuracy", value = accuracy)
recall = mat_conf[2,2]/sum(mat_conf[,2])
results[[3]]$output[[2]] = list(metric = "recall", value = recall)
precision = mat_conf[2,2]/sum(mat_conf[2,])
results[[3]]$output[[3]] = list(metric = "precision", value = precision)
precision = mat_conf[2,2]/sum(mat_conf[2,])
recall = mat_conf[2,2]/sum(mat_conf[,2])
F1 = 2*precision*recall/(precision+recall)
results[[3]]$output[[4]] = list(metric = "f1 score", value = F1)
 first_row = mat_conf[1,1] / (mat_conf[1,1] + mat_conf[1,2])  
 second_row <- mat_conf[2,2] / (mat_conf[2,1] + mat_conf[2,2])  
 balanced_accuracy = (first_row + second_row)/2
results[[3]]$output[[5]] = list(metric = "balanced accuracy", value = balanced_accuracy)
print(writeLines(toJSON(results)))
